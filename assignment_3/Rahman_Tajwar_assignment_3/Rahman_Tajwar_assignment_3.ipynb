{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_file, test_file):\n",
    "    # Read and process the training data\n",
    "    with open(train_file, 'r', encoding='utf-8') as file:\n",
    "        train_lines = file.readlines()\n",
    "\n",
    "    # Lowercase and count word occurrences\n",
    "    word_count = {}\n",
    "    train_processed = []\n",
    "    for line in train_lines:\n",
    "        # Pad and lowercase the line\n",
    "        padded_line = '<s> ' + line.strip().lower() + ' </s>'\n",
    "        train_processed.append(padded_line)\n",
    "        \n",
    "        # Count words\n",
    "        words = padded_line.split()\n",
    "        for word in words:\n",
    "            if word not in word_count:\n",
    "                word_count[word] = 0\n",
    "            word_count[word] += 1\n",
    "\n",
    "    # Replace single-occurrence words with <unk>\n",
    "    train_final = []\n",
    "    for line in train_processed:\n",
    "        new_line = ' '.join('<unk>' if word_count[word] == 1 else word for word in line.split())\n",
    "        train_final.append(new_line)\n",
    "\n",
    "    # Process the test data, replacing unseen words with <unk>\n",
    "    with open(test_file, 'r', encoding='utf-8') as file:\n",
    "        test_lines = file.readlines()\n",
    "\n",
    "    test_processed = []\n",
    "    for line in test_lines:\n",
    "        # Pad and lowercase the line\n",
    "        padded_line = '<s> ' + line.strip().lower() + ' </s>'\n",
    "        # Replace unseen words\n",
    "        new_line = ' '.join('<unk>' if word not in word_count else word for word in padded_line.split())\n",
    "        test_processed.append(new_line)\n",
    "\n",
    "    train_split = [train_sentence.split() for train_sentence in train_final]\n",
    "    test_split = [test_sentence.split() for test_sentence in test_processed]\n",
    "    return train_split, test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'brown.train.txt'\n",
    "test_file = 'brown.test.txt'\n",
    "\n",
    "train_data, test_data = preprocess_data(train_file, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Samples:\n",
      "['<s>', 'the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.', '</s>']\n",
      "['<s>', 'the', '<unk>', 'term', 'jury', 'had', 'been', 'charged', 'by', 'fulton', 'superior', 'court', 'judge', '<unk>', '<unk>', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', '<unk>', 'ivan', 'allen', 'jr.', '.', '</s>']\n",
      "['<s>', 'the', 'jury', 'said', 'it', 'did', 'find', 'that', 'many', 'of', \"georgia's\", 'registration', 'and', 'election', 'laws', '``', 'are', 'outmoded', 'or', 'inadequate', 'and', 'often', 'ambiguous', \"''\", '.', '</s>']\n",
      "['<s>', 'it', 'recommended', 'that', 'fulton', 'legislators', 'act', '``', 'to', 'have', 'these', 'laws', 'studied', 'and', 'revised', 'to', 'the', 'end', 'of', 'modernizing', 'and', 'improving', 'them', \"''\", '.', '</s>']\n",
      "['<s>', 'the', 'grand', 'jury', 'commented', 'on', 'a', 'number', 'of', 'other', 'topics', ',', 'among', 'them', 'the', 'atlanta', 'and', 'fulton', 'county', 'purchasing', 'departments', 'which', 'it', 'said', '``', 'are', 'well', 'operated', 'and', 'follow', 'generally', 'accepted', 'practices', 'which', 'inure', 'to', 'the', 'best', 'interest', 'of', 'both', 'governments', \"''\", '.', '</s>']\n",
      "\n",
      "Test Data Samples:\n",
      "['<s>', 'the', 'jury', 'further', 'said', 'in', '<unk>', '<unk>', 'that', 'the', 'city', 'executive', 'committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'city', 'of', 'atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.', '</s>']\n",
      "['<s>', '``', 'only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', \"''\", ',', 'the', 'jury', 'said', ',', '``', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', ',', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city', \"''\", '.', '</s>']\n",
      "['<s>', 'the', 'jury', 'did', 'not', 'elaborate', ',', 'but', 'it', 'added', 'that', '``', 'there', 'should', 'be', 'periodic', 'surveillance', 'of', 'the', 'pricing', 'practices', 'of', 'the', 'concessionaires', 'for', 'the', 'purpose', 'of', 'keeping', 'the', 'prices', 'reasonable', \"''\", '.', '</s>']\n",
      "['<s>', 'on', 'other', 'matters', ',', 'the', 'jury', 'recommended', 'that', ':', '(', '1', ')', '</s>']\n",
      "['<s>', 'they', 'have', 'a', 'son', ',', 'william', 'berry', 'jr.', ',', 'and', 'a', 'daughter', ',', 'mrs.', 'j.', 'm.', '<unk>', 'of', 'griffin', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# prints out 5 lines from the processed test and training txt file\n",
    "def print_txt(data, num_samples=5):\n",
    "    for i in range(num_samples):\n",
    "        print(data[i])\n",
    "\n",
    "\n",
    "# Display processed training and test data\n",
    "print(\"Training Data Samples:\")\n",
    "print_txt(train_data)\n",
    "print(\"\\nTest Data Samples:\")\n",
    "print_txt(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Training the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to generate unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of Tokens:  1018784\n",
      "Total # of Unique Words:  24796\n"
     ]
    }
   ],
   "source": [
    "# Helper function to generate unigrams and bigrams\n",
    "def generate_ngrams(sentences):\n",
    "    unigrams = Counter()\n",
    "    bigrams = defaultdict(Counter)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Generate unigrams\n",
    "        for word in sentence:\n",
    "            unigrams[word] += 1\n",
    "        # Generate bigrams\n",
    "        for w1, w2 in zip(sentence[:-1], sentence[1:]):\n",
    "            bigrams[w1][w2] += 1\n",
    "\n",
    "    return unigrams, bigrams\n",
    "\n",
    "\n",
    "# Generate the unigrams and bigrams from the sentences\n",
    "unigrams, bigrams = generate_ngrams(train_data)\n",
    "\n",
    "# Total number of words (tokens) in the corpus\n",
    "total_tokens = sum(unigrams.values())\n",
    "print(\"Total # of Tokens: \", total_tokens)\n",
    "\n",
    "# Vocabulary size\n",
    "vocabulary_size = len(unigrams)\n",
    "print(\"Total # of Unique Words: \", vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram Maximum Likelihood Estimate\n",
    "\n",
    "def unigram_mle(word):\n",
    "    return unigrams[word] / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Maximum Likelihood Estimate\n",
    "\n",
    "def bigram_mle(w1, w2):\n",
    "    if unigrams[w1] == 0: return 0\n",
    "    return bigrams[w1][w2] / unigrams[w1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Add-One Smoothing\n",
    "\n",
    "def bigram_add_one(w1, w2):\n",
    "    return (bigrams[w1][w2] + 1) / (unigrams[w1] + vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Katz Backoff\n",
    "\n",
    "def bigram_katz_backoff(w1, w2, discount=0.5):\n",
    "    bigram_count = bigrams[w1][w2]\n",
    "    unigram_count = unigrams[w1]\n",
    "\n",
    "    # Epsilon is a small value to handle unseen words and avoid division by zero\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    if bigram_count > 0:\n",
    "        return max((bigram_count - discount) / (unigram_count if unigram_count > 0 else epsilon), epsilon)\n",
    "    else:\n",
    "        # Calculate backoff weight only if w1 is seen\n",
    "        backoff_weight = (discount * len(bigrams[w1]) / (unigram_count if unigram_count > 0 else epsilon)) if unigram_count > 0 else epsilon\n",
    "        unigram_probability = unigrams[w2] / (total_tokens if total_tokens > 0 else epsilon)\n",
    "        return max(backoff_weight * unigram_probability, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3: Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return [line.strip().lower() for line in file]\n",
    "\n",
    "\n",
    "def count_words(corpus):\n",
    "    word_count = Counter()\n",
    "    for line in corpus:\n",
    "        words = line.split()\n",
    "        word_count.update(words)\n",
    "    return word_count\n",
    "\n",
    "\n",
    "train_corpus = read_corpus(train_file)\n",
    "test_corpus = read_corpus(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Number of Word Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of word types in training\n",
    "train_word_count = count_words(train_corpus)\n",
    "# Adding the unknown token\n",
    "train_word_count['<unk>'] += 1\n",
    "\n",
    "# Count unique word types including padding symbols and the unknown token\n",
    "train_types_including_padding_and_unk = len(train_word_count)\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "# Number of word tokens in training\n",
    "\n",
    "# The sum of the values in train_word_count gives the number of tokens\n",
    "train_tokens_including_padding = sum(train_word_count.values())\n",
    "\n",
    "\n",
    "######################################################################################## \n",
    "# Percent of unseen word tokens + types in test\n",
    "test_word_count = count_words(test_corpus)\n",
    "\n",
    "# Identify unseen words and count unseen tokens\n",
    "unseen_test_types = set(test_word_count.keys()) - set(train_word_count.keys())\n",
    "unseen_test_tokens_count = sum(test_word_count[word] for word in unseen_test_types)\n",
    "\n",
    "# Calculate percentages\n",
    "percentage_unseen_test_types = (len(unseen_test_types) / len(test_word_count)) * 100\n",
    "percentage_unseen_test_tokens = (unseen_test_tokens_count / sum(test_word_count.values())) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word types in the training corpus (including padding and <unk>): 44978\n",
      "Number of word tokens in the training corpus (including padding): 927141\n",
      "Percentage of unseen word types in test data: 15.63%\n",
      "Percentage of unseen word tokens in test data: 2.19%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of word types in the training corpus (including padding and <unk>): {train_types_including_padding_and_unk}\")\n",
    "print(f\"Number of word tokens in the training corpus (including padding): {train_tokens_including_padding}\")\n",
    "print(f\"Percentage of unseen word types in test data: {percentage_unseen_test_types:.2f}%\")\n",
    "print(f\"Percentage of unseen word tokens in test data: {percentage_unseen_test_tokens:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: <s> he was laughed off the screen . </s>\n",
      "Unigram Model Log Probability: -50.00253069510074\n",
      "Bigram Model Log Probability: -29.226429204762596\n",
      "Bigram with Add-One Smoothing Log Probability: -48.567846423407694\n",
      "\n",
      "Sentence: <s> there was no compulsion behind them . </s>\n",
      "Unigram Model Log Probability: -53.82193870947094\n",
      "Bigram Model Log Probability: -24.057083130827618\n",
      "Bigram with Add-One Smoothing Log Probability: -46.04452641155733\n",
      "\n",
      "Sentence: <s> i look forward to hearing your reply . </s>\n",
      "Unigram Model Log Probability: -63.4380424815825\n",
      "Bigram Model Log Probability: -inf\n",
      "Bigram with Add-One Smoothing Log Probability: -66.80947224624923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Define the sentences\n",
    "sentences = [\n",
    "    \"<s> he was laughed off the screen . </s>\",\n",
    "    \"<s> there was no compulsion behind them . </s>\",\n",
    "    \"<s> i look forward to hearing your reply . </s>\"\n",
    "]\n",
    "\n",
    "# Assumed precomputed counts from training data\n",
    "# Assuming dictionaries `unigrams` and `bigrams` are already filled from previous code\n",
    "# Assuming `vocabulary_size` and `total_tokens` are defined from training data preprocessing\n",
    "\n",
    "\n",
    "def compute_unigram_log_prob(sentence, unigrams, total_tokens):\n",
    "    words = sentence.split()\n",
    "    log_prob = sum(math.log(unigrams[word] / total_tokens)for word in words)\n",
    "    return log_prob\n",
    "\n",
    "\n",
    "def compute_bigram_log_prob(sentence, unigrams, bigrams):\n",
    "    words = sentence.split()\n",
    "    log_prob = 0\n",
    "    for w1, w2 in zip(words[:-1], words[1:]):\n",
    "        bigram_count = bigrams[w1][w2]\n",
    "        if bigram_count == 0:\n",
    "            # Log probability is -inf if any bigram count is zero\n",
    "            return float('-inf')\n",
    "        log_prob += math.log(bigram_count / unigrams[w1])\n",
    "    return log_prob\n",
    "\n",
    "\n",
    "def compute_bigram_add_one_log_prob(sentence, unigrams, bigrams, vocabulary_size):\n",
    "    words = sentence.split()\n",
    "    log_prob = 0\n",
    "    for w1, w2 in zip(words[:-1], words[1:]):\n",
    "        bigram_count = bigrams[w1][w2]\n",
    "        unigram_count = unigrams[w1]\n",
    "        log_prob += math.log((bigram_count + 1) / (unigram_count + vocabulary_size))\n",
    "    return log_prob\n",
    "\n",
    "\n",
    "# Compute and display the log probabilities\n",
    "for sentence in sentences:\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    unigram_log_prob = compute_unigram_log_prob(sentence, unigrams, total_tokens)\n",
    "    bigram_log_prob = compute_bigram_log_prob(sentence, unigrams, bigrams)\n",
    "    bigram_add_one_log_prob = compute_bigram_add_one_log_prob(sentence, unigrams, bigrams, vocabulary_size)\n",
    "    print(f\"Unigram Model Log Probability: {unigram_log_prob}\")\n",
    "    print(f\"Bigram Model Log Probability: {bigram_log_prob}\")\n",
    "    print(f\"Bigram with Add-One Smoothing Log Probability: {bigram_add_one_log_prob}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: <s> he was laughed off the screen . </s>\n",
      "Unigram Model Perplexity: 258.74\n",
      "Bigram Model Perplexity: 25.72288397506204\n",
      "Bigram with Add-One Smoothing Perplexity: 220.62\n",
      "\n",
      "Sentence: <s> there was no compulsion behind them . </s>\n",
      "Unigram Model Perplexity: 395.53\n",
      "Bigram Model Perplexity: 14.483487925135439\n",
      "Bigram with Add-One Smoothing Perplexity: 166.68\n",
      "\n",
      "Sentence: <s> i look forward to hearing your reply . </s>\n",
      "Unigram Model Perplexity: 568.96\n",
      "Bigram Model Perplexity: inf\n",
      "Bigram with Add-One Smoothing Perplexity: 797.07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_unigram_perplexity(sentence, unigrams, total_tokens):\n",
    "    words = sentence.split()\n",
    "    log_prob = sum(math.log2(unigrams[word] / total_tokens)for word in words)\n",
    "    perplexity = 2 ** (-log_prob / len(words))\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def compute_bigram_perplexity(sentence, unigrams, bigrams):\n",
    "    words = sentence.split()\n",
    "    log_probs = []\n",
    "    for w1, w2 in zip(words[:-1], words[1:]):\n",
    "        bigram_count = bigrams[w1][w2]\n",
    "        if bigram_count == 0:\n",
    "            # Perplexity is infinite if any bigram count is zero\n",
    "            return float('inf')\n",
    "        log_probs.append(math.log2(bigram_count / unigrams[w1]))\n",
    "    perplexity = 2 ** (-sum(log_probs) / len(words))\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def compute_bigram_add_one_perplexity(sentence, unigrams, bigrams, vocabulary_size):\n",
    "    words = sentence.split()\n",
    "    log_probs = []\n",
    "    for w1, w2 in zip(words[:-1], words[1:]):\n",
    "        bigram_count = bigrams[w1][w2]\n",
    "        unigram_count = unigrams[w1]\n",
    "        log_probs.append(math.log2((bigram_count + 1) /(unigram_count + vocabulary_size)))\n",
    "    perplexity = 2 ** (-sum(log_probs) / len(words))\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "# Compute and display the perplexities\n",
    "sentences = [\n",
    "    \"<s> he was laughed off the screen . </s>\",\n",
    "    \"<s> there was no compulsion behind them . </s>\",\n",
    "    \"<s> i look forward to hearing your reply . </s>\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    unigram_perplexity = compute_unigram_perplexity(sentence, unigrams, total_tokens)\n",
    "    bigram_perplexity = compute_bigram_perplexity(sentence, unigrams, bigrams)\n",
    "    bigram_add_one_perplexity = compute_bigram_add_one_perplexity(sentence, unigrams, bigrams, vocabulary_size)\n",
    "    print(f\"Unigram Model Perplexity: {unigram_perplexity:.2f}\")\n",
    "    print(f\"Bigram Model Perplexity: {bigram_perplexity if bigram_perplexity != float('inf') else 'inf'}\")\n",
    "    print(f\"Bigram with Add-One Smoothing Perplexity: {bigram_add_one_perplexity:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Model Perplexity for the Test Corpus: 1261.2473476641203\n",
      "Bigram Model Perplexity for the Test Corpus: inf\n",
      "Bigram with Add-One Smoothing Perplexity for the Test Corpus: 1156.4532096440262\n"
     ]
    }
   ],
   "source": [
    "def safe_log(x):\n",
    "    if x <= 0:\n",
    "        return float('-inf')\n",
    "    return math.log(x)\n",
    "\n",
    "# Calculate total log probability for unigram model\n",
    "\n",
    "\n",
    "def unigram_corpus_log_prob(corpus, unigrams, total_tokens):\n",
    "    total_log_prob = 0\n",
    "    word_count = 0\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            prob = unigrams.get(word, 1) / total_tokens\n",
    "            total_log_prob += safe_log(prob)\n",
    "            word_count += 1\n",
    "    return total_log_prob, word_count\n",
    "\n",
    "# Calculate total log probability for bigram model\n",
    "\n",
    "\n",
    "def bigram_corpus_log_prob(corpus, unigrams, bigrams):\n",
    "    total_log_prob = 0\n",
    "    word_count = 0\n",
    "    for sentence in corpus:\n",
    "        for w1, w2 in zip(sentence[:-1], sentence[1:]):\n",
    "            bigram_count = bigrams.get((w1, w2), 0)\n",
    "            unigram_count = unigrams.get(w1, 1)\n",
    "            prob = bigram_count / unigram_count if unigram_count != 0 else 0\n",
    "            total_log_prob += safe_log(prob)\n",
    "            word_count += 1\n",
    "    return total_log_prob, word_count\n",
    "\n",
    "# Calculate total log probability for bigram model with Add-One smoothing\n",
    "\n",
    "\n",
    "def bigram_add_one_corpus_log_prob(corpus, unigrams, bigrams, vocabulary_size):\n",
    "    total_log_prob = 0\n",
    "    word_count = 0\n",
    "    for sentence in corpus:\n",
    "        for w1, w2 in zip(sentence[:-1], sentence[1:]):\n",
    "            bigram_count = bigrams.get((w1, w2), 0)\n",
    "            unigram_count = unigrams.get(w1, 0)\n",
    "            prob = (bigram_count + 1) / (unigram_count + vocabulary_size)\n",
    "            total_log_prob += safe_log(prob)\n",
    "            word_count += 1\n",
    "    return total_log_prob, word_count\n",
    "\n",
    "# Calculate perplexity for the entire corpus\n",
    "\n",
    "\n",
    "def corpus_perplexity(total_log_prob, total_word_count):\n",
    "    return 2 ** (-total_log_prob / total_word_count)\n",
    "\n",
    "\n",
    "# Compute log probabilities and word counts\n",
    "total_log_prob_unigram, total_words_unigram = unigram_corpus_log_prob(\n",
    "    test_corpus, unigrams, total_tokens)\n",
    "total_log_prob_bigram, total_words_bigram = bigram_corpus_log_prob( test_corpus, unigrams, bigrams)\n",
    "total_log_prob_bigram_add_one, total_words_bigram_add_one = bigram_add_one_corpus_log_prob(test_corpus, unigrams, bigrams, vocabulary_size)\n",
    "\n",
    "# Compute perplexities\n",
    "perplexity_unigram = corpus_perplexity(total_log_prob_unigram, total_words_unigram)\n",
    "perplexity_bigram = corpus_perplexity(total_log_prob_bigram, total_words_bigram)\n",
    "perplexity_bigram_add_one = corpus_perplexity(total_log_prob_bigram_add_one, total_words_bigram_add_one)\n",
    "\n",
    "print(f\"Unigram Model Perplexity for the Test Corpus: {perplexity_unigram}\")\n",
    "print(f\"Bigram Model Perplexity for the Test Corpus: {perplexity_bigram if perplexity_bigram != float('inf') else 'inf'}\")\n",
    "print(f\"Bigram with Add-One Smoothing Perplexity for the Test Corpus: {perplexity_bigram_add_one}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Conclusions\n",
    "\n",
    "#### Overview\n",
    "In this project, we analyzed a training corpus consisting of 927,141 word tokens and 44,978 unique word types (including padding symbols and a special `<unk>` token for unknown words). We applied three statistical language models — unigram, bigram, and bigram with Add-One smoothing — to compute the log probabilities and perplexities of various test sentences. Additionally, we explored the model performances across the entire test corpus.\n",
    "\n",
    "#### Vocabulary and Coverage\n",
    "The training corpus showed a rich vocabulary with nearly 45,000 unique types. However, when applying the models to the test data, 15.63% of word types and 2.19% of word tokens were unseen in the training data. This indicates a significant diversity in language usage between the training and test sets, which is a common challenge in language modeling.\n",
    "\n",
    "#### Sentence-wise Analysis\n",
    "For individual sentences, the log probability and perplexity metrics highlighted the strengths and weaknesses of each model:\n",
    "\n",
    "- **Sentence: \"He was laughed off the screen.\"**\n",
    "  - The bigram model significantly outperformed the unigram model in terms of lower perplexity (25.72 vs. 258.74), indicating better context sensitivity.\n",
    "  - The Add-One smoothing increased the perplexity relative to the bigram model, showing the trade-off between smoothing and model precision.\n",
    "\n",
    "- **Sentence: \"There was no compulsion behind them.\"**\n",
    "  - Again, the bigram model achieved much lower perplexity (14.48) compared to the unigram model (395.53), highlighting its effectiveness in handling specific phrase structures.\n",
    "  - The smoothed bigram model showed improved performance over the unigram but was not as effective as the unigram model in capturing sentence structure.\n",
    "\n",
    "- **Sentence: \"I look forward to hearing your reply.\"**\n",
    "  - This sentence revealed limitations in the bigram model, which registered an infinite perplexity due to encountering an unseen bigram combination, showing the model's vulnerability to data sparsity.\n",
    "  - The smoothed bigram model's high perplexity (797.07) indicates that even smoothing techniques may not always compensate adequately for sparse data.\n",
    "\n",
    "#### Overall Corpus Analysis\n",
    "- **Unigram Model Perplexity:** 1261.25\n",
    "  - Indicates poor predictive performance across the test corpus, likely due to its ignorance of word order and context.\n",
    "  \n",
    "- **Bigram Model Perplexity:** ∞\n",
    "  - Suggests the presence of many unseen bigrams in the test data, leading to infinite perplexity and highlighting the model's inability to handle new word combinations effectively without smoothing.\n",
    "  \n",
    "- **Bigram with Add-One Smoothing Perplexity:** 1156.45\n",
    "  - Although better than the unigram model, the high perplexity indicates that even with smoothing, the model struggles with the diversity and complexity of the test corpus.\n",
    "\n",
    "### Conclusion\n",
    "The experiment underscores the critical importance of context in language processing, as demonstrated by the superior performance of the bigram model over the unigram model in most scenarios where the data was seen during training. However, the presence of unseen data poses significant challenges, particularly for the non-smoothed bigram model. The use of smoothing techniques, like Add-One smoothing, but may not be sufficient for highly diverse datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
