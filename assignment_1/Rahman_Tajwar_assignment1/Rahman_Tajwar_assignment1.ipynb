{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute Force KNN for Binary Classification\n",
    "\n",
    "Creating a KNN from scratch and testing it on the wine quality dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculates the Euclidean distance between two vectors. The Euclidean distance is the \"ordinary\" straight-line\n",
    "    distance between two points in Euclidean space. This measure is often used in machine learning to assess\n",
    "    the similarity between vectors, with a wide range of applications in clustering, classification, and more.\n",
    "\n",
    "    Parameters:\n",
    "    - vec1 (list or array-like): The first vector.\n",
    "    - vec2 (list or array-like): The second vector, which must be of the same length as vec1.\n",
    "\n",
    "    Returns:\n",
    "    - float: The Euclidean distance between the two vectors.\n",
    "    \"\"\"\n",
    "    # Ensure the vectors are of the same length\n",
    "    if len(vec1) != len(vec2):\n",
    "        raise ValueError(\"Vectors must be of the same length\")\n",
    "\n",
    "    # Calculate the squared differences and sum them\n",
    "    squared_diffs = [(v1 - v2) ** 2 for v1, v2 in zip(vec1, vec2)]\n",
    "    sum_squared_diffs = sum(squared_diffs)\n",
    "\n",
    "    # Return the square root of the sum of squared differences\n",
    "    return math.sqrt(sum_squared_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculates the Manhattan distance between two vectors. The Manhattan distance is defined as the sum\n",
    "    of the absolute differences between corresponding elements of the vectors. This distance metric is\n",
    "    often used in geometry, coding theory, and many other fields.\n",
    "\n",
    "    Parameters:\n",
    "    - vec1 (list or array-like): The first vector.\n",
    "    - vec2 (list or array-like): The second vector. It must be of the same length as vec1.\n",
    "\n",
    "    Returns:\n",
    "    - int or float: The Manhattan distance between the two vectors.\n",
    "    \"\"\"\n",
    "    # Ensure the vectors are of the same length\n",
    "    if len(vec1) != len(vec2):\n",
    "        raise ValueError(\"Vectors must be of the same length\")\n",
    "\n",
    "    # Calculate the absolute differences and sum them\n",
    "    abs_diffs = [abs(v1 - v2) for v1, v2 in zip(vec1, vec2)]\n",
    "    return sum(abs_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_and_error(vec_predictions, vec_actual):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy and generalization error (interpreted as error rate) for a set of predictions\n",
    "    against the actual labels in a binary classification task. Accuracy is the proportion of correct predictions\n",
    "    over the total number of predictions, and the error rate is calculated as 1 minus the accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    - vec_predictions (list or array-like): Predicted labels by the classification model.\n",
    "    - vec_actual (list or array-like): Actual labels from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing two floats:\n",
    "        - accuracy (float): The accuracy of the predictions.\n",
    "        - error_rate (float): The generalization error, calculated as 1 minus the accuracy.\n",
    "    \"\"\"\n",
    "    # Ensure the vectors are of the same length\n",
    "    if len(vec_predictions) != len(vec_actual):\n",
    "        raise ValueError(\"Vectors must be of the same length\")\n",
    "\n",
    "    # For every prediction that is equal to the actual value, add 1 and find the sum\n",
    "    correct_predictions = sum(1 for pred, actual in zip(\n",
    "        vec_predictions, vec_actual) if pred == actual)\n",
    "    total_predictions = len(vec_predictions)\n",
    "    # Calculate the proportion of correct predictions\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # Calculate error rate as 1 - accuracy\n",
    "    error_rate = 1 - accuracy\n",
    "\n",
    "    return accuracy, error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(true_positives, false_positives):\n",
    "    \"\"\"\n",
    "    Calculates the precision metric for a binary classification task.\n",
    "    Precision is defined as the ratio of true positives to the sum of true positives and false positives.\n",
    "\n",
    "    Parameters:\n",
    "    - true_positives (int): The count of true positive predictions.\n",
    "    - false_positives (int): The count of false positive predictions.\n",
    "\n",
    "    Returns:\n",
    "    - precision (float): The calculated precision value. Returns 0 if the denominator is 0, preventing division by zero.\n",
    "    \"\"\"\n",
    "    if true_positives + false_positives == 0:\n",
    "        return 0\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "\n",
    "def recall(true_positives, false_negatives):\n",
    "    \"\"\"\n",
    "    Calculates the recall metric for a binary classification task.\n",
    "    Recall is defined as the ratio of true positives to the sum of true positives and false negatives.\n",
    "\n",
    "    Parameters:\n",
    "    - true_positives (int): The count of true positive predictions.\n",
    "    - false_negatives (int): The count of false negative predictions.\n",
    "\n",
    "    Returns:\n",
    "    - recall (float): The calculated recall value. Returns 0 if the denominator is 0, preventing division by zero.\n",
    "    \"\"\"\n",
    "    if true_positives + false_negatives == 0:\n",
    "        return 0\n",
    "    return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    \"\"\"\n",
    "    Calculates the F1 score for a binary classification task.\n",
    "    The F1 score is the harmonic mean of precision and recall.\n",
    "\n",
    "    Parameters:\n",
    "    - precision (float): The precision value.\n",
    "    - recall (float): The recall value.\n",
    "\n",
    "    Returns:\n",
    "    - F1 Score (float): The calculated F1 score. Returns 0 if the sum of precision and recall is 0, preventing division by zero.\n",
    "    \"\"\"\n",
    "    # Fixing the original formula to correctly calculate the F1 score\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return (2 * precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(vec_actual, vec_predictions):\n",
    "    \"\"\"\n",
    "    Computes the confusion matrix for a set of predictions against the actual labels\n",
    "    for a binary classification task. The confusion matrix is represented as a tuple\n",
    "    containing counts of true negatives, false positives, false negatives, and true positives.\n",
    "\n",
    "    Parameters:\n",
    "    - vec_actual (list): Actual labels from the dataset, where 1 represents\n",
    "      the positive class and 0 represents the negative class.\n",
    "    - vec_predictions (list): Predicted labels by the classification model,\n",
    "      where 1 represents a prediction for the positive class and 0 for the negative class.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple of four integers: (true_neg, false_pos, false_neg, true_pos) representing the\n",
    "      counts of true negatives, false positives, false negatives, and true positives, respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    # cover the case where the denominator is 0\n",
    "    if len(vec_actual) != len(vec_predictions):\n",
    "        return ValueError(\"Vectors must be of the same length\")\n",
    "    \n",
    "    true_pos, true_neg, false_pos, false_neg = 0, 0, 0, 0\n",
    "    for actual, pred in zip(vec_actual, vec_predictions):\n",
    "        if actual == 1 and pred == 1: true_pos += 1\n",
    "        elif actual == 1 and pred == 0: false_neg += 1\n",
    "        elif actual == 0 and pred == 1: false_pos += 1\n",
    "        else: true_neg += 1\n",
    "        \n",
    "    return true_neg, false_pos, false_neg, true_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve(actual, predictions): #TODO might need to convert to numpy arr\n",
    "    \"\"\"\n",
    "    Compute ROC curve data.\n",
    "    \n",
    "    Parameters:\n",
    "    - actual (list): of true binary labels.\n",
    "    - predictions (list): of scores or probabilities for the positive class.\n",
    "    \n",
    "    Returns:\n",
    "    - fpr: Array of false positive rates.\n",
    "    - tpr: Array of true positive rates.\n",
    "    - thresholds: Array of thresholds used.\n",
    "    \"\"\"\n",
    "    thresholds = sorted(predictions)[::-1]\n",
    "    fpr, tpr = [], []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        # new predictions based on the threshold\n",
    "        preds = [p for p in predictions if p >= threshold ]\n",
    "        \n",
    "        # call the confusion matrix function\n",
    "        tn, fp, fn, tp = confusion_matrix(actual, preds)\n",
    "        \n",
    "        current_tpr = tp/(tp+fn) if (tp + fn) > 0 else 0\n",
    "        current_fpr = fp/(fp+tn) if (fp + tn) > 0 else 0\n",
    "        \n",
    "        tpr.append(current_tpr)\n",
    "        fpr.append(current_fpr)\n",
    "        \n",
    "    # return fpr and tpr in an increasing order\n",
    "    return sorted(fpr), sorted(tpr), thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_roc(fpr, tpr):\n",
    "    \"\"\"\n",
    "    Calculates the Area Under the Curve (AUC) for the Receiver Operating Characteristic (ROC) curve.\n",
    "    This function uses the trapezoidal rule to approximate the integral under the ROC curve, which\n",
    "    represents the AUC. The AUC is a measure of the ability of a classifier to distinguish between\n",
    "    classes and is used as a summary of the ROC curve.\n",
    "\n",
    "    Parameters:\n",
    "    - fpr (list): An array of false positive rates, sorted in increasing order.\n",
    "    - tpr (list): An array of true positive rates, sorted in increasing order.\n",
    "\n",
    "    Returns:\n",
    "    - auc (float): The approximated Area Under the Curve (AUC) value.\n",
    "    \"\"\"\n",
    "    # Ensure the inputs are of the same length\n",
    "    if len(fpr) != len(tpr):\n",
    "        raise ValueError(\"The length of FPR and TPR arrays must be the same\")\n",
    "\n",
    "    # Calculate the AUC using the trapezoidal rule\n",
    "    auc = 0.0\n",
    "    for i in range(1, len(fpr)):\n",
    "        # The area of each trapezoid is calculated and added to the total AUC\n",
    "        auc += (fpr[i] - fpr[i-1]) * (tpr[i] + tpr[i-1]) / 2\n",
    "\n",
    "    return auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
